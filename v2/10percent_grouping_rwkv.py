#!/usr/bin/env python3
"""
–î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–∫–µ—Ä–Ω—ã—Ö —Ä—É–∫ —Å RWKV –º–æ–¥–µ–ª—è–º–∏
–ú–æ–¥–µ–ª—å 1: –ü—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø–æ –ø–æ–∑–∏—Ü–∏—è–º
–ú–æ–¥–µ–ª—å 2: –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–ª–∞ —Ä—É–∫–∏ —Å —É—á–µ—Ç–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
"""

import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import json
from datetime import datetime
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–º—É –∫–æ–¥—É
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∫–æ–¥–∞
from hand_range_prediction import (
    PokerHandEvaluator,
    add_hand_evaluation_to_dataframe,
    setup_directories,
    find_data_files,
    choose_data_file,
    combine_all_data_files,
    RWKV_Block,
    ImprovedRWKVBlock,
    PokerHandAnalyzer,
    prepare_features,
    safe_json_serialize
)

# ===================== –ú–û–î–ï–õ–¨ 1: –ü–†–ï–§–õ–û–ü –î–ò–ê–ü–ê–ó–û–ù–´ =====================

class PreflopRangeRWKV(nn.Module):
    """
    RWKV –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∫–∞—Ä—Ç –Ω–∞ –ø—Ä–µ—Ñ–ª–æ–ø–µ
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Ä—É–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
    """
    def __init__(self, input_dim, hidden_dim=256, num_layers=3, num_positions=9):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_positions = num_positions
        self.num_starting_hands = 169  # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ —Ä—É–∫–∏ –≤ —Ö–æ–ª–¥–µ–º–µ
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–∑–∏—Ü–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π
        self.position_embedding = nn.Embedding(num_positions, 32)
        self.action_embedding = nn.Embedding(10, 32)  # fold, call, raise –∏ —Ç.–¥.
        
        # –í—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        self.input_projection = nn.Linear(input_dim + 64, hidden_dim)
        
        # RWKV –±–ª–æ–∫–∏ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∫–æ–¥–∞
        self.rwkv_blocks = nn.ModuleList([
            RWKV_Block(hidden_dim) for _ in range(num_layers)
        ])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ dropout
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.2)
        
        # –í—ã—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
        self.range_heads = nn.ModuleDict({
            f'pos_{i}': nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim // 2, self.num_starting_hands),
                nn.Sigmoid()  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Ä—É–∫–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            ) for i in range(num_positions)
        })
        
    def reset_states(self):
        """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏–π RWKV –±–ª–æ–∫–æ–≤"""
        for block in self.rwkv_blocks:
            block.reset_state()
        
    def forward(self, x, positions, actions, mask=None):
        batch_size, seq_len, _ = x.shape
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        pos_emb = self.position_embedding(positions)
        act_emb = self.action_embedding(actions)
        x = torch.cat([x, pos_emb, act_emb], dim=-1)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è
        x = self.input_projection(x)
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ RWKV –±–ª–æ–∫–∏
        for block in self.rwkv_blocks:
            x = block(x)
            x = self.layer_norm(x)
            x = self.dropout(x)
        
        # –î–ª—è –ø—Ä–µ—Ñ–ª–æ–ø–∞ –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–µ—Ñ–ª–æ–ø –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        preflop_state = x[:, -1, :]  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø—Ä–µ—Ñ–ª–æ–ø - —ç—Ç–æ –Ω–∞—á–∞–ª–æ
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
        range_predictions = {}
        for pos in range(self.num_positions):
            range_predictions[f'pos_{pos}'] = self.range_heads[f'pos_{pos}'](preflop_state)
        
        return range_predictions

# ===================== –ú–û–î–ï–õ–¨ 2: –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–ò–õ–ê –†–£–ö–ò =====================

class ProbableHandRWKV(nn.Module):
    """
    RWKV –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≥—Ä—É–ø–ø—ã —Å–∏–ª—ã —Ä—É–∫–∏ (1-10)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –æ—Ç –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
    """
    def __init__(self, input_dim, hidden_dim=384, num_layers=4, num_groups=10):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_groups = num_groups
        self.range_dim = 169 * 9  # 169 —Ä—É–∫ * 9 –ø–æ–∑–∏—Ü–∏–π
        
        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
        self.range_encoder = nn.Sequential(
            nn.Linear(self.range_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128)
        )
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.street_embedding = nn.Embedding(5, 32)  # preflop, flop, turn, river, showdown
        self.action_embedding = nn.Embedding(10, 32)
        
        # –í—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è (–≤–∫–ª—é—á–∞–µ—Ç –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã)
        self.input_projection = nn.Linear(input_dim + 128 + 64, hidden_dim)
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ RWKV –±–ª–æ–∫–∏ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∫–æ–¥–∞
        self.rwkv_blocks = nn.ModuleList([
            ImprovedRWKVBlock(hidden_dim) for _ in range(num_layers)
        ])
        
        # Attention –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.2)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.group_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, num_groups)
        )
        
    def forward(self, x, range_predictions, streets, actions, mask=None):
        batch_size, seq_len, _ = x.shape
        
        # –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä
        range_flat = []
        for pos in range(9):
            if f'pos_{pos}' in range_predictions:
                range_flat.append(range_predictions[f'pos_{pos}'])
        
        if range_flat:
            range_vector = torch.cat(range_flat, dim=-1)
            range_encoded = self.range_encoder(range_vector)
            # –†–∞—Å—à–∏—Ä—è–µ–º –Ω–∞ –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            range_encoded = range_encoded.unsqueeze(1).expand(-1, seq_len, -1)
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω—É–ª–∏
            range_encoded = torch.zeros(batch_size, seq_len, 128).to(x.device)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        street_emb = self.street_embedding(streets)
        action_emb = self.action_embedding(actions)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        x = torch.cat([x, range_encoded, street_emb, action_emb], dim=-1)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è
        x = self.input_projection(x)
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ RWKV –±–ª–æ–∫–∏
        for block in self.rwkv_blocks:
            x = block(x, mask)
            x = self.layer_norm(x)
            x = self.dropout(x)
        
        # Self-attention –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
        x, _ = self.attention(x, x, x, key_padding_mask=mask)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–∞–ª–∏–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        if mask is not None:
            lengths = (~mask).sum(dim=1)
            batch_indices = torch.arange(batch_size).to(x.device)
            final_states = x[batch_indices, lengths - 1]
        else:
            final_states = x[:, -1, :]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã
        group_logits = self.group_classifier(final_states)
        
        return group_logits

# ===================== –î–ê–¢–ê–°–ï–¢–´ =====================

class PreflopRangeDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô)"""
    def __init__(self, sequences, feature_columns, targets, scaler=None):
        self.sequences = sequences
        self.feature_columns = feature_columns
        self.targets = targets  # –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
        self.scaler = scaler
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        self.valid_indices = []
        for i, seq in enumerate(sequences):
            if len(seq) > 0:
                self.valid_indices.append(i)
        
        print(f"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(self.valid_indices)} –∏–∑ {len(sequences)}")
        
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        seq_df = self.sequences[actual_idx]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ—Ñ–ª–æ–ø –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        preflop_mask = seq_df['Street_id'] == 0  # –ü—Ä–µ—Ñ–ª–æ–ø
        preflop_seq = seq_df[preflop_mask]
        
        if len(preflop_seq) == 0:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ—Ñ–ª–æ–ø –¥–∞–Ω–Ω—ã—Ö, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏
            preflop_seq = seq_df.iloc[:min(5, len(seq_df))]
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ –¥–ª—è scaler
        if self.scaler:
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ –≤—Å–µ–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
            features_df = pd.DataFrame(columns=self.feature_columns)
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            for col in self.feature_columns:
                if col in preflop_seq.columns:
                    features_df[col] = preflop_seq[col].values
                else:
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω—É–ª—è–º–∏
                    features_df[col] = 0
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª—è–º–∏
            features_df = features_df.fillna(0)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler –∫ DataFrame
            features = self.scaler.transform(features_df)
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç scaler, –ø—Ä–æ—Å—Ç–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            available_features = [col for col in self.feature_columns if col in preflop_seq.columns]
            features = preflop_seq[available_features].fillna(0).values
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω—É–ª—è–º–∏
            if len(available_features) < len(self.feature_columns):
                full_features = np.zeros((len(preflop_seq), len(self.feature_columns)))
                for i, col in enumerate(self.feature_columns):
                    if col in available_features:
                        col_idx = available_features.index(col)
                        full_features[:, i] = features[:, col_idx]
                features = full_features
        
        features = features.astype(np.float32)
        
        # –ü–æ–∑–∏—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è –∏–≥—Ä–æ–∫–æ–≤
        positions = preflop_seq['Position_encoded'].values if 'Position_encoded' in preflop_seq else np.zeros(len(preflop_seq))
        actions = preflop_seq['Action_encoded'].values if 'Action_encoded' in preflop_seq else np.zeros(len(preflop_seq))
        
        # –¶–µ–ª–µ–≤—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        target_ranges = self.targets.get(actual_idx, {})
        
        return {
            'features': torch.tensor(features, dtype=torch.float32),
            'positions': torch.tensor(positions, dtype=torch.long),
            'actions': torch.tensor(actions, dtype=torch.long),
            'target_ranges': target_ranges,
            'seq_length': len(preflop_seq)
        }

def collate_fn(batch):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π —Å None –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)"""
    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
    batch = [item for item in batch if item is not None]
    if len(batch) == 0:
        return None
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    max_len = max(item['seq_length'] for item in batch)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–ª—è –±–∞—Ç—á–∞
    padded_features = []
    padded_positions = []
    padded_actions = []
    target_ranges = []
    seq_lengths = []
    
    for item in batch:
        seq_len = item['seq_length']
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        features = item['features']
        positions = item['positions']
        actions = item['actions']
        
        # Padding features
        if seq_len < max_len:
            # –î–æ–±–∞–≤–ª—è–µ–º padding
            feat_padding = torch.zeros(max_len - seq_len, features.shape[1])
            features = torch.cat([features, feat_padding], dim=0)
            
            # Padding –¥–ª—è positions –∏ actions
            pos_padding = torch.zeros(max_len - seq_len, dtype=torch.long)
            positions = torch.cat([positions, pos_padding], dim=0)
            
            act_padding = torch.zeros(max_len - seq_len, dtype=torch.long)
            actions = torch.cat([actions, act_padding], dim=0)
        
        padded_features.append(features)
        padded_positions.append(positions)
        padded_actions.append(actions)
        target_ranges.append(item['target_ranges'])
        seq_lengths.append(seq_len)
    
    # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á
    batch_dict = {
        'features': torch.stack(padded_features),
        'positions': torch.stack(padded_positions),
        'actions': torch.stack(padded_actions),
        'target_ranges': target_ranges,
        'seq_lengths': torch.tensor(seq_lengths)
    }
    
    return batch_dict
class ProbableHandDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–∏–ª—ã —Ä—É–∫–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô)"""
    def __init__(self, sequences, feature_columns, range_predictions, targets, scaler=None, max_seq_length=50):
        self.sequences = sequences
        self.feature_columns = feature_columns
        self.range_predictions = range_predictions  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
        self.targets = targets  # –ì—Ä—É–ø–ø—ã 0-9 –∏–∑ HM3 –º–∞–ø–ø–∏–Ω–≥–∞
        self.scaler = scaler
        self.max_seq_length = max_seq_length
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
        self.valid_indices = []
        for i, seq in enumerate(sequences):
            if len(seq) > 0 and i < len(targets):
                self.valid_indices.append(i)
        
        print(f"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(self.valid_indices)} –∏–∑ {len(sequences)}")
        
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        seq_df = self.sequences[actual_idx]
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ –¥–ª—è scaler
        if self.scaler:
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ –≤—Å–µ–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
            features_df = pd.DataFrame(columns=self.feature_columns)
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            for col in self.feature_columns:
                if col in seq_df.columns:
                    features_df[col] = seq_df[col].values
                else:
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω—É–ª—è–º–∏
                    features_df[col] = 0
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª—è–º–∏
            features_df = features_df.fillna(0)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler –∫ DataFrame
            features = self.scaler.transform(features_df)
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç scaler, –ø—Ä–æ—Å—Ç–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            available_features = [col for col in self.feature_columns if col in seq_df.columns]
            features = seq_df[available_features].fillna(0).values
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω—É–ª—è–º–∏
            if len(available_features) < len(self.feature_columns):
                full_features = np.zeros((len(seq_df), len(self.feature_columns)))
                for i, col in enumerate(self.feature_columns):
                    if col in available_features:
                        col_idx = available_features.index(col)
                        full_features[:, i] = features[:, col_idx]
                features = full_features
        
        features = features.astype(np.float32)
        
        # Padding/truncation
        seq_len = min(len(features), self.max_seq_length)
        if seq_len < self.max_seq_length:
            padding = np.zeros((self.max_seq_length - seq_len, features.shape[1]))
            features = np.vstack([features[:seq_len], padding])
        else:
            features = features[:self.max_seq_length]
        
        # –£–ª–∏—Ü—ã –∏ –¥–µ–π—Å—Ç–≤–∏—è
        streets = seq_df['Street_id'].values[:seq_len] if 'Street_id' in seq_df else np.zeros(seq_len)
        actions = seq_df['Action_encoded'].values[:seq_len] if 'Action_encoded' in seq_df else np.zeros(seq_len)
        
        # Padding –¥–ª—è streets –∏ actions
        if seq_len < self.max_seq_length:
            streets = np.pad(streets, (0, self.max_seq_length - seq_len), 'constant')
            actions = np.pad(actions, (0, self.max_seq_length - seq_len), 'constant')
        
        # –ú–∞—Å–∫–∞ –¥–ª—è padding
        mask = torch.zeros(self.max_seq_length, dtype=torch.bool)
        mask[seq_len:] = True
        
        # –¶–µ–ª–µ–≤–∞—è –≥—Ä—É–ø–ø–∞
        target_group = self.targets[actual_idx] if actual_idx < len(self.targets) else 0
        
        return {
            'features': torch.tensor(features, dtype=torch.float32),
            'range_predictions': self.range_predictions.get(actual_idx, {}),
            'streets': torch.tensor(streets, dtype=torch.long),
            'actions': torch.tensor(actions, dtype=torch.long),
            'mask': mask,
            'target_group': torch.tensor(target_group, dtype=torch.long),
            'seq_length': seq_len
        }

# ===================== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• =====================

def prepare_feature_columns(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    feature_columns = [
        'Level', 'Pot', 'Stack', 'SPR', 'Street_id', 'Round',
        'ActionOrder', 'Seat', 'Dealer', 'Bet', 'Allin',
        'PlayerWins', 'WinAmount'
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ä—Ç—ã —Å—Ç–æ–ª–∞
    analyzer = PokerHandAnalyzer()
    board_columns = ['Card1', 'Card2', 'Card3', 'Card4', 'Card5']
    for col in board_columns:
        if col in df.columns:
            df[f'{col}_rank'], df[f'{col}_suit'] = zip(
                *df[col].apply(
                    lambda x: analyzer.parse_card(x) if pd.notna(x) else (0, 0)
                )
            )
            feature_columns.extend([f'{col}_rank', f'{col}_suit'])
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    encoders = {}
    
    if 'Position' in df.columns:
        encoders['position'] = LabelEncoder()
        df['Position_encoded'] = encoders['position'].fit_transform(
            df['Position'].fillna('Unknown')
        )
        feature_columns.append('Position_encoded')
    
    if 'Action' in df.columns:
        encoders['action'] = LabelEncoder()
        df['Action_encoded'] = encoders['action'].fit_transform(
            df['Action'].fillna('Unknown')
        )
        feature_columns.append('Action_encoded')
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    available_features = [col for col in feature_columns if col in df.columns]
    
    return available_features, encoders

def prepare_two_stage_data(file_path, hm3_mapping_path):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
    """
    print("üéØ === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –î–í–£–•–≠–¢–ê–ü–ù–û–ô –ú–û–î–ï–õ–ò ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(file_path)
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ HM3 –º–∞–ø–ø–∏–Ω–≥–∞
    with open(hm3_mapping_path, 'r') as f:
        hm3_mapping = json.load(f)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π —Å —à–æ—É–¥–∞—É–Ω–æ–º
    mask = (df['Showdown_1'].notna()) & (df['Showdown_2'].notna())
    df_filtered = df[mask].copy()
    print(f"üÉè –ù–∞–π–¥–µ–Ω–æ {len(df_filtered)} –∑–∞–ø–∏—Å–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏")
    
    if len(df_filtered) == 0:
        print("‚ùå –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏!")
        return None
    
    # –î–æ–±–∞–≤–ª—è–µ–º HM3 –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
    df_filtered = add_hand_evaluation_to_dataframe(df_filtered)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º 10% –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É
    df_filtered['target_group'] = df_filtered['hand_type_hm3'].map(hm3_mapping['mapping'])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –≤—ã–≤–æ–¥–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø
    print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ —Å–∏–ª—ã (10 –≥—Ä—É–ø–ø):")
    group_counts = df_filtered['target_group'].value_counts().sort_index()
    for group, count in group_counts.items():
        print(f"   –ì—Ä—É–ø–ø–∞ {group}: {count} ({count/len(df_filtered)*100:.1f}%)")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_columns, encoders = prepare_feature_columns(df_filtered)
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Ä—É–∫–∞–º
    sequences = create_hand_sequences(df_filtered)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:")
    seq_lengths = [len(seq) for seq in sequences]
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {np.mean(seq_lengths):.1f}")
    print(f"   –ú–∏–Ω/–ú–∞–∫—Å: {min(seq_lengths)}/{max(seq_lengths)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
    train_sequences, val_sequences, test_sequences = split_sequences(sequences)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ scaler –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö train –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
    all_train_data = []
    for seq in train_sequences:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        available_cols = [col for col in feature_columns if col in seq.columns]
        if available_cols:
            all_train_data.append(seq[available_cols])
    
    if all_train_data:
        all_train_df = pd.concat(all_train_data, ignore_index=True)
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ fit
        all_train_df = all_train_df.fillna(0)
        scaler.fit(all_train_df)
    else:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler!")
        return None
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏
    preflop_targets = create_preflop_targets(train_sequences)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    final_targets_train = create_final_targets(train_sequences, hm3_mapping)
    final_targets_val = create_final_targets(val_sequences, hm3_mapping)
    final_targets_test = create_final_targets(test_sequences, hm3_mapping)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    print(f"\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
    print(f"   Train targets: {len(final_targets_train)}")
    print(f"   Val targets: {len(final_targets_val)}")
    print(f"   Test targets: {len(final_targets_test)}")
    
    return {
        'train_sequences': train_sequences,
        'val_sequences': val_sequences,
        'test_sequences': test_sequences,
        'feature_columns': feature_columns,
        'preflop_targets': preflop_targets,
        'final_targets': {
            'train': final_targets_train,
            'val': final_targets_val,
            'test': final_targets_test
        },
        'hm3_mapping': hm3_mapping,
        'scaler': scaler,
        'encoders': encoders,
        'df_filtered': df_filtered  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    }

def create_hand_sequences(df):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π —Ä—É–∫–∏"""
    sequences = []
    
    hand_col = 'HandID' if 'HandID' in df.columns else 'Hand'
    
    for hand_id in df[hand_col].unique():
        hand_data = df[df[hand_col] == hand_id].copy()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–ª–∏—Ü–∞–º –∏ –ø–æ—Ä—è–¥–∫—É –¥–µ–π—Å—Ç–≤–∏–π
        hand_data = hand_data.sort_values(['Street_id', 'ActionOrder'])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        hand_data['hand_id'] = hand_id
        hand_data['sequence_position'] = range(len(hand_data))
        
        sequences.append(hand_data)
    
    print(f"üìã –°–æ–∑–¥–∞–Ω–æ {len(sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ä—É–∫")
    return sequences

def split_sequences(sequences, test_size=0.15, val_size=0.15, random_state=42):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –Ω–∞ train/val/test"""
    np.random.seed(random_state)
    
    n_sequences = len(sequences)
    indices = np.random.permutation(n_sequences)
    
    n_test = int(n_sequences * test_size)
    n_val = int(n_sequences * val_size)
    
    test_indices = indices[:n_test]
    val_indices = indices[n_test:n_test + n_val]
    train_indices = indices[n_test + n_val:]
    
    train_sequences = [sequences[i] for i in train_indices]
    val_sequences = [sequences[i] for i in val_indices]
    test_sequences = [sequences[i] for i in test_indices]
    
    print(f"‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   Train: {len(train_sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print(f"   Val: {len(val_sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print(f"   Test: {len(test_sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    
    return train_sequences, val_sequences, test_sequences

def create_preflop_targets(sequences):
    """
    –°–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏
    –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ —Ä—É–∫–∏ –¥–æ—Ö–æ–¥—è—Ç –¥–æ —Ñ–ª–æ–ø–∞
    """
    position_ranges = defaultdict(lambda: defaultdict(int))
    position_counts = defaultdict(int)
    
    for seq in sequences:
        # –ù–∞—Ö–æ–¥–∏–º –∏–≥—Ä–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ—à–ª–∏ –¥–æ —Ñ–ª–æ–ø–∞
        if 'Street_id' in seq.columns:
            flop_players = seq[seq['Street_id'] >= 1]['PlayerID'].unique() if 'PlayerID' in seq else []
        else:
            continue
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–≥—Ä–æ–∫–∞ –Ω–∞ –ø—Ä–µ—Ñ–ª–æ–ø–µ
        preflop_data = seq[seq['Street_id'] == 0]
        
        for _, row in preflop_data.iterrows():
            if 'PlayerID' in row and row['PlayerID'] in flop_players:
                position = row.get('Position_encoded', 0)
                cards = (row.get('Showdown_1'), row.get('Showdown_2'))
                
                if pd.notna(cards[0]) and pd.notna(cards[1]):
                    hand_index = cards_to_index(cards[0], cards[1])
                    position_ranges[position][hand_index] += 1
                    position_counts[position] += 1
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    normalized_ranges = {}
    for pos in position_ranges:
        total = position_counts[pos]
        if total > 0:
            normalized_ranges[pos] = {
                hand: count / total 
                for hand, count in position_ranges[pos].items()
            }
    
    return normalized_ranges

def cards_to_index(card1, card2):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–≤–µ –∫–∞—Ä—Ç—ã –≤ –∏–Ω–¥–µ–∫—Å —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Ä—É–∫–∏ (0-168)"""
    rank_map = {'2': 0, '3': 1, '4': 2, '5': 3, '6': 4, '7': 5, '8': 6, 
                '9': 7, 'T': 8, 'J': 9, 'Q': 10, 'K': 11, 'A': 12}
    
    r1 = rank_map.get(card1[0], 0)
    r2 = rank_map.get(card2[0], 0)
    suited = card1[1] == card2[1]
    
    # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Ä–∞–Ω–≥–∏
    if r1 < r2:
        r1, r2 = r2, r1
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω–¥–µ–∫—Å
    if r1 == r2:  # –ü–∞—Ä–∞
        return r1
    elif suited:  # –û–¥–Ω–æ–º–∞—Å—Ç–Ω—ã–µ
        return 13 + r1 * 13 + r2
    else:  # –†–∞–∑–Ω–æ–º–∞—Å—Ç–Ω—ã–µ
        return 91 + r1 * 13 + r2

def create_final_targets(sequences, hm3_mapping):
    """–°–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –≥—Ä—É–ø–ø—ã –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)"""
    targets = []
    
    for seq in sequences:
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä—É–∫–µ
        last_row = seq.iloc[-1]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if 'hand_type_hm3' in last_row and pd.notna(last_row['hand_type_hm3']):
            # –ü–æ–ª—É—á–∞–µ–º –≥—Ä—É–ø–ø—É –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞
            hand_type = last_row['hand_type_hm3']
            group = hm3_mapping['mapping'].get(hand_type, 9)
            targets.append(group)
        elif 'target_group' in last_row and pd.notna(last_row['target_group']):
            # –ï—Å–ª–∏ –≥—Ä—É–ø–ø–∞ —É–∂–µ –≤—ã—á–∏—Å–ª–µ–Ω–∞
            targets.append(int(last_row['target_group']))
        else:
            # Default –≥—Ä—É–ø–ø–∞
            targets.append(9)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    unique_targets = np.unique(targets)
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã –≤ targets: {unique_targets}")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {dict(zip(*np.unique(targets, return_counts=True)))}")
    
    return targets

# ===================== –û–ë–£–ß–ï–ù–ò–ï =====================

def train_two_stage_system(data_dict):
    """
    –û–±—É—á–µ–Ω–∏–µ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    """
    print("\nüöÄ === –û–ë–£–ß–ï–ù–ò–ï –î–í–£–•–≠–¢–ê–ü–ù–û–ô –°–ò–°–¢–ï–ú–´ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –≠—Ç–∞–ø 1: –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏
    print("\nüìå –≠–¢–ê–ü 1: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤")
    preflop_model = train_preflop_model(data_dict, device)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüîÆ –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤...")
    range_predictions = get_range_predictions(preflop_model, data_dict, device)
    
    # –≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\nüìå –≠–¢–ê–ü 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–∏–ª—ã —Ä—É–∫–∏")
    final_model = train_final_model(data_dict, range_predictions, device)
    
    return preflop_model, final_model, range_predictions

def train_preflop_model(data_dict, device):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = PreflopRangeDataset(
        data_dict['train_sequences'],
        data_dict['feature_columns'],
        data_dict['preflop_targets'],
        data_dict['scaler']
    )
    
    if len(train_dataset) == 0:
        print("‚ùå –ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏!")
        return None
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=16,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π batch size
        shuffle=True,
        collate_fn=collate_fn,
        drop_last=True  # –£–±–∏—Ä–∞–µ–º –Ω–µ–ø–æ–ª–Ω—ã–µ –±–∞—Ç—á–∏
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = PreflopRangeRWKV(
        input_dim=len(data_dict['feature_columns']),
        hidden_dim=128,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        num_layers=2     # –ú–µ–Ω—å—à–µ —Å–ª–æ–µ–≤
    ).to(device)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –±–æ–ª—å—à–∏–º learning rate
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    mse_criterion = nn.MSELoss()
    bce_criterion = nn.BCELoss()
    
    # –û–±—É—á–µ–Ω–∏–µ
    epochs = 20
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        batch_count = 0
        
        for batch in train_loader:
            if batch is None:
                continue
                
            features = batch['features'].to(device)
            positions = batch['positions'].to(device)
            actions = batch['actions'].to(device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            if features.size(0) == 0:
                continue
            
            optimizer.zero_grad()
            model.reset_states()
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            try:
                range_preds = model(features, positions, actions)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ forward pass: {e}")
                continue
            
            # –†–∞—Å—á–µ—Ç –ø–æ—Ç–µ—Ä—å
            loss = 0
            loss_count = 0
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø–æ—Ç–µ—Ä—å - –ø—Ä–æ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–∞—á–∞–ª–∞
            for batch_idx in range(features.size(0)):
                for pos in range(9):
                    if f'pos_{pos}' in range_preds:
                        pred = range_preds[f'pos_{pos}'][batch_idx]
                        
                        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ü–µ–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
                        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞—Å—Ç–æ—è—â–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
                        target_vector = torch.zeros(169).to(device)
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                        random_indices = torch.randint(0, 169, (20,))
                        target_vector[random_indices] = torch.rand(20).to(device)
                        target_vector = target_vector / target_vector.sum()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                        
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MSE –≤–º–µ—Å—Ç–æ BCE –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        loss += mse_criterion(pred, target_vector)
                        loss_count += 1
            
            if loss_count > 0:
                loss = loss / loss_count
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é
                l2_reg = 0
                for param in model.parameters():
                    l2_reg += torch.sum(param ** 2)
                loss += 0.001 * l2_reg
                
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                total_loss += loss.item()
                batch_count += 1
        
        avg_loss = total_loss / max(batch_count, 1)
        if epoch % 5 == 0:
            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
    
    return model

def train_final_model(data_dict, range_predictions, device):
    """–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = ProbableHandDataset(
        data_dict['train_sequences'],
        data_dict['feature_columns'],
        range_predictions['train'],
        data_dict['final_targets']['train'],
        data_dict['scaler']
    )
    
    val_dataset = ProbableHandDataset(
        data_dict['val_sequences'],
        data_dict['feature_columns'],
        range_predictions['val'],
        data_dict['final_targets']['val'],
        data_dict['scaler']
    )
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = ProbableHandRWKV(
        input_dim=len(data_dict['feature_columns']),
        hidden_dim=384,
        num_layers=4
    ).to(device)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = optim.AdamW(model.parameters(), lr=0.0008)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)
    criterion = nn.CrossEntropyLoss()
    
    best_val_accuracy = 0
    best_model_state = None
    
    # –û–±—É—á–µ–Ω–∏–µ
    epochs = 30
    for epoch in range(epochs):
        # Training
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch in train_loader:
            features = batch['features'].to(device)
            range_preds = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                          for k, v in batch['range_predictions'].items()}
            streets = batch['streets'].to(device)
            actions = batch['actions'].to(device)
            mask = batch['mask'].to(device)
            targets = batch['target_group'].to(device)
            
            optimizer.zero_grad()
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            group_logits = model(features, range_preds, streets, actions, mask)
            
            # –ü–æ—Ç–µ—Ä–∏
            loss = criterion(group_logits, targets)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # –¢–æ—á–Ω–æ—Å—Ç—å
            _, predicted = torch.max(group_logits, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
        
        train_accuracy = correct / total
        
        # Validation
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch in val_loader:
                features = batch['features'].to(device)
                range_preds = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                              for k, v in batch['range_predictions'].items()}
                streets = batch['streets'].to(device)
                actions = batch['actions'].to(device)
                mask = batch['mask'].to(device)
                targets = batch['target_group'].to(device)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                group_logits = model(features, range_preds, streets, actions, mask)
                
                # –ü–æ—Ç–µ—Ä–∏
                loss = criterion(group_logits, targets)
                val_loss += loss.item()
                
                # –¢–æ—á–Ω–æ—Å—Ç—å
                _, predicted = torch.max(group_logits, 1)
                val_total += targets.size(0)
                val_correct += (predicted == targets).sum().item()
        
        val_accuracy = val_correct / val_total
        avg_val_loss = val_loss / len(val_loader)
        
        print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, "
              f"Train Loss: {total_loss/len(train_loader):.4f}, "
              f"Train Acc: {train_accuracy:.3f}, "
              f"Val Loss: {avg_val_loss:.4f}, "
              f"Val Acc: {val_accuracy:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_model_state = model.state_dict().copy()
        
        scheduler.step(avg_val_loss)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é: {best_val_accuracy:.3f}")
    
    return model

def get_range_predictions(preflop_model, data_dict, device):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    preflop_model.eval()
    predictions = {'train': {}, 'val': {}, 'test': {}}
    
    for split in ['train', 'val', 'test']:
        sequences = data_dict[f'{split}_sequences']
        
        dataset = PreflopRangeDataset(
            sequences,
            data_dict['feature_columns'],
            {},  # –ü—É—Å—Ç—ã–µ targets –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            data_dict['scaler']
        )
        
        loader = DataLoader(
            dataset, 
            batch_size=64, 
            shuffle=False,
            collate_fn=collate_fn
        )
        
        split_predictions = {}
        idx = 0
        
        with torch.no_grad():
            for batch in loader:
                if batch is None:
                    continue
                    
                features = batch['features'].to(device)
                positions = batch['positions'].to(device)
                actions = batch['actions'].to(device)
                
                preflop_model.reset_states()
                range_preds = preflop_model(features, positions, actions)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ
                batch_size = features.shape[0]
                for i in range(batch_size):
                    split_predictions[idx] = {
                        pos: range_preds[pos][i].cpu()
                        for pos in range_preds
                    }
                    idx += 1
        
        predictions[split] = split_predictions
    
    return predictions

# ===================== –û–¶–ï–ù–ö–ê –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø =====================

def evaluate_two_stage_system(preflop_model, final_model, data_dict, range_predictions):
    """
    –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    """
    print("\nüìä === –û–¶–ï–ù–ö–ê –î–í–£–•–≠–¢–ê–ü–ù–û–ô –°–ò–°–¢–ï–ú–´ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # –û—Ü–µ–Ω–∫–∞ –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏
    print("\nüìå –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤:")
    preflop_metrics = evaluate_preflop_model(preflop_model, data_dict, device)
    
    # –û—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    print("\nüìå –û—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    final_metrics = evaluate_final_model(final_model, data_dict, range_predictions, device)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    visualize_results(preflop_metrics, final_metrics, data_dict)
    
    return {
        'preflop': preflop_metrics,
        'final': final_metrics
    }

def evaluate_preflop_model(model, data_dict, device):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤"""
    model.eval()
    metrics = {}
    
    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
    # –ù–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–ø-10% —Ä—É–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
    
    return metrics

def evaluate_final_model(model, data_dict, range_predictions, device):
    """–û—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model.eval()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    test_dataset = ProbableHandDataset(
        data_dict['test_sequences'],
        data_dict['feature_columns'],
        range_predictions['test'],
        data_dict['final_targets']['test'],
        data_dict['scaler']
    )
    
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch in test_loader:
            features = batch['features'].to(device)
            range_preds = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                          for k, v in batch['range_predictions'].items()}
            streets = batch['streets'].to(device)
            actions = batch['actions'].to(device)
            mask = batch['mask'].to(device)
            targets = batch['target_group']
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            group_logits = model(features, range_preds, streets, actions, mask)
            _, predicted = torch.max(group_logits, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.numpy())
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
    
    accuracy = accuracy_score(all_targets, all_predictions)
    cm = confusion_matrix(all_targets, all_predictions)
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø
    group_distribution = {i: 0 for i in range(10)}
    for pred in all_predictions:
        group_distribution[pred] += 1
    
    metrics = {
        'overall_accuracy': accuracy,
        'confusion_matrix': cm,
        'group_distribution': group_distribution,
        'predictions': all_predictions,
        'targets': all_targets
    }
    
    print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {accuracy:.3f}")
    
    return metrics

def visualize_results(preflop_metrics, final_metrics, data_dict):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    ax1 = axes[0, 0]
    cm = final_metrics['confusion_matrix']
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar_kws={'label': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'})
    ax1.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∞')
    ax1.set_ylabel('–ò—Å—Ç–∏–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∞')
    ax1.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (10 –≥—Ä—É–ø–ø HM3)')
    ax1.set_xticklabels([f'G{i+1}' for i in range(10)])
    ax1.set_yticklabels([f'G{i+1}' for i in range(10)])
    
    # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø
    ax2 = axes[0, 1]
    group_distribution = final_metrics['group_distribution']
    
    groups = list(range(10))
    counts = [group_distribution.get(i, 0) for i in groups]
    colors = plt.cm.RdYlGn_r(np.linspace(0, 1, 10))
    
    bars = ax2.bar(groups, counts, color=colors)
    ax2.set_xlabel('–ì—Ä—É–ø–ø–∞')
    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    ax2.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø')
    ax2.set_xticks(groups)
    ax2.set_xticklabels([f'G{i+1}' for i in groups])
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
    total = sum(counts)
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{count/total*100:.1f}%', ha='center', va='bottom')
    
    # 3. –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –≥—Ä—É–ø–ø–∞–º
    ax3 = axes[0, 2]
    group_accuracies = []
    
    for i in range(10):
        mask = np.array(final_metrics['targets']) == i
        if mask.sum() > 0:
            group_preds = np.array(final_metrics['predictions'])[mask]
            group_targets = np.array(final_metrics['targets'])[mask]
            acc = (group_preds == group_targets).mean()
            group_accuracies.append(acc)
        else:
            group_accuracies.append(0)
    
    bars = ax3.bar(groups, group_accuracies, color=colors)
    ax3.set_xlabel('–ì—Ä—É–ø–ø–∞')
    ax3.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    ax3.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º')
    ax3.set_xticks(groups)
    ax3.set_xticklabels([f'G{i+1}' for i in groups])
    ax3.set_ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
    for bar, acc in zip(bars, group_accuracies):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{acc:.2f}', ha='center', va='bottom')
    
    # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    ax4 = axes[1, 0]
    
    true_distribution = {i: 0 for i in range(10)}
    for target in final_metrics['targets']:
        true_distribution[target] += 1
    
    x = np.arange(10)
    width = 0.35
    
    true_counts = [true_distribution[i] for i in range(10)]
    pred_counts = [final_metrics['group_distribution'][i] for i in range(10)]
    
    bars1 = ax4.bar(x - width/2, true_counts, width, label='–ò—Å—Ç–∏–Ω–Ω–æ–µ', alpha=0.8)
    bars2 = ax4.bar(x + width/2, pred_counts, width, label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ', alpha=0.8)
    
    ax4.set_xlabel('–ì—Ä—É–ø–ø–∞')
    ax4.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    ax4.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π')
    ax4.set_xticks(x)
    ax4.set_xticklabels([f'G{i+1}' for i in range(10)])
    ax4.legend()
    
    # 5. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏
    ax5 = axes[1, 1]
    
    true_groups = final_metrics['targets']
    pred_groups = final_metrics['predictions']
    
    ax5.scatter(true_groups, pred_groups, alpha=0.5)
    ax5.plot([0, 9], [0, 9], 'r--', lw=2)  # –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è
    ax5.set_xlabel('–ò—Å—Ç–∏–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∞')
    ax5.set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∞')
    ax5.set_title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')
    ax5.set_xticks(range(10))
    ax5.set_yticks(range(10))
    ax5.grid(True, alpha=0.3)
    
    # 6. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    ax6 = axes[1, 2]
    ax6.axis('off')
    
    stats_text = f"""
    –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
    
    ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {final_metrics['overall_accuracy']:.3f}
    ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(final_metrics['targets'])}
    ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø: 10
    
    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥—Ä—É–ø–ø–∞–º —Å–∏–ª—ã:
    ‚Ä¢ G1-G2: –ú–æ–Ω—Å—Ç—Ä—ã (—Å–∏–ª—å–Ω–µ–π—à–∏–µ)
    ‚Ä¢ G3-G4: –°–∏–ª—å–Ω—ã–µ —Ä—É–∫–∏
    ‚Ä¢ G5-G6: –°—Ä–µ–¥–Ω–∏–µ —Ä—É–∫–∏
    ‚Ä¢ G7-G8: –°–ª–∞–±—ã–µ —Ä—É–∫–∏
    ‚Ä¢ G9-G10: –ú—É—Å–æ—Ä/–î—Ä–æ
    """
    
    ax6.text(0.1, 0.9, stats_text, transform=ax6.transAxes,
             fontsize=12, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    plt.savefig(f'two_stage_results_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: two_stage_results_{timestamp}.png")

# ===================== –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô =====================

def save_two_stage_models(preflop_model, final_model, metrics):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏
    preflop_path = f'models/preflop_range_model_{timestamp}.pth'
    torch.save({
        'model_state': preflop_model.state_dict(),
        'model_config': {
            'input_dim': preflop_model.input_dim,
            'hidden_dim': preflop_model.hidden_dim,
            'num_layers': len(preflop_model.rwkv_blocks),
            'num_positions': preflop_model.num_positions
        },
        'metrics': metrics.get('preflop', {}),
        'timestamp': timestamp
    }, preflop_path)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    final_path = f'models/final_hand_model_{timestamp}.pth'
    torch.save({
        'model_state': final_model.state_dict(),
        'model_config': {
            'input_dim': final_model.input_dim,
            'hidden_dim': final_model.hidden_dim,
            'num_layers': len(final_model.rwkv_blocks),
            'num_groups': final_model.num_groups
        },
        'metrics': metrics.get('final', {}),
        'timestamp': timestamp
    }, final_path)
    
    print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(f"   üìÅ –ü—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª—å: {preflop_path}")
    print(f"   üìÅ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_path}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report = {
        'timestamp': timestamp,
        'preflop_model': preflop_path,
        'final_model': final_path,
        'metrics': {
            'preflop': metrics.get('preflop', {}),
            'final': safe_json_serialize(metrics.get('final', {}))
        },
        'overall_accuracy': metrics['final']['overall_accuracy']
    }
    
    report_path = f'results/two_stage_report_{timestamp}.json'
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"   üìã –û—Ç—á–µ—Ç: {report_path}")

# ===================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø =====================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    
    print("üé∞ === –î–í–£–•–≠–¢–ê–ü–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ü–û–ö–ï–†–ù–´–• –†–£–ö ===\n")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    setup_directories()
    
    # –í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
    data_file = choose_data_file()
    if not data_file:
        return
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—ã–±—Ä–∞–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    if data_file == "COMBINE_ALL":
        print("\nüîó === –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –§–ê–ô–õ–û–í ===")
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
        result = combine_all_data_files()
        if result is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ñ–∞–π–ª—ã")
            return
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        data_file, combination_summary = result
        print(f"\n‚úÖ –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {data_file}")
        print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {combination_summary['total_records']:,}")
        print(f"üÉè –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–æ—É–¥–∞—É–Ω–æ–≤: {combination_summary['total_showdowns']:,}")
    
    # –ü—É—Ç—å –∫ –º–∞–ø–ø–∏–Ω–≥—É HM3
    hm3_mapping_path = 'hm3_10pct_simple_mapping_20250612_072153.json'
    
    if not os.path.exists(hm3_mapping_path):
        print(f"‚ùå –§–∞–π–ª –º–∞–ø–ø–∏–Ω–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {hm3_mapping_path}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        return
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data_dict = prepare_two_stage_data(data_file, hm3_mapping_path)
    
    if data_dict is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return
    
    # –û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    preflop_model, final_model, range_predictions = train_two_stage_system(data_dict)
    
    # –û—Ü–µ–Ω–∫–∞
    metrics = evaluate_two_stage_system(preflop_model, final_model, data_dict, range_predictions)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    save_two_stage_models(preflop_model, final_model, metrics)
    
    print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {metrics['final']['overall_accuracy']:.3f}")

if __name__ == "__main__":
    main()