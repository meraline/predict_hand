def prepare_two_stage_data(file_path, hm3_mapping_path):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
    """
    print("üéØ === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –î–í–£–•–≠–¢–ê–ü–ù–û–ô –ú–û–î–ï–õ–ò ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(file_path)
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ HM3 –º–∞–ø–ø–∏–Ω–≥–∞
    with open(hm3_mapping_path, 'r') as f:
        hm3_mapping = json.load(f)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π —Å —à–æ—É–¥–∞—É–Ω–æ–º
    mask = (df['Showdown_1'].notna()) & (df['Showdown_2'].notna())
    df_filtered = df[mask].copy()
    print(f"üÉè –ù–∞–π–¥–µ–Ω–æ {len(df_filtered)} –∑–∞–ø–∏—Å–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏")
    
    if len(df_filtered) == 0:
        print("‚ùå –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏!")
        return None
    
    # –î–æ–±–∞–≤–ª—è–µ–º HM3 –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
    df_filtered = add_hand_evaluation_to_dataframe(df_filtered)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º 10% –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É
    df_filtered['target_group'] = df_filtered['hand_type_hm3'].map(hm3_mapping['mapping'])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –≤—ã–≤–æ–¥–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø
    print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ —Å–∏–ª—ã (10 –≥—Ä—É–ø–ø):")
    group_counts = df_filtered['target_group'].value_counts().sort_index()
    for group, count in group_counts.items():
        print(f"   –ì—Ä—É–ø–ø–∞ {group}: {count} ({count/len(df_filtered)*100:.1f}%)")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_columns, encoders = prepare_feature_columns(df_filtered)
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Ä—É–∫–∞–º
    sequences = create_hand_sequences(df_filtered)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:")
    seq_lengths = [len(seq) for seq in sequences]
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {np.mean(seq_lengths):.1f}")
    print(f"   –ú–∏–Ω/–ú–∞–∫—Å: {min(seq_lengths)}/{max(seq_lengths)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
    train_sequences, val_sequences, test_sequences = split_sequences(sequences)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ scaler –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö train –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
    all_train_data = []
    for seq in train_sequences:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        available_cols = [col for col in feature_columns if col in seq.columns]
        if available_cols:
            all_train_data.append(seq[available_cols])
    
    if all_train_data:
        all_train_df = pd.concat(all_train_data, ignore_index=True)
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ fit
        all_train_df = all_train_df.fillna(0)
        scaler.fit(all_train_df)
    else:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler!")
        return None
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏
    preflop_targets = create_preflop_targets(train_sequences)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    final_targets_train = create_final_targets(train_sequences, hm3_mapping)
    final_targets_val = create_final_targets(val_sequences, hm3_mapping)
    final_targets_test = create_final_targets(test_sequences, hm3_mapping)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    print(f"\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
    print(f"   Train targets: {len(final_targets_train)}")
    print(f"   Val targets: {len(final_targets_val)}")
    print(f"   Test targets: {len(final_targets_test)}")
    
    return {
        'train_sequences': train_sequences,
        'val_sequences': val_sequences,
        'test_sequences': test_sequences,
        'feature_columns': feature_columns,
        'preflop_targets': preflop_targets,
        'final_targets': {
            'train': final_targets_train,
            'val': final_targets_val,
            'test': final_targets_test
        },
        'hm3_mapping': hm3_mapping,
        'scaler': scaler,
        'encoders': encoders,
        'df_filtered': df_filtered  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    }
    
    
def create_final_targets(sequences, hm3_mapping):
    """–°–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –≥—Ä—É–ø–ø—ã –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)"""
    targets = []
    
    for seq in sequences:
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä—É–∫–µ
        last_row = seq.iloc[-1]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if 'hand_type_hm3' in last_row and pd.notna(last_row['hand_type_hm3']):
            # –ü–æ–ª—É—á–∞–µ–º –≥—Ä—É–ø–ø—É –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞
            hand_type = last_row['hand_type_hm3']
            group = hm3_mapping['mapping'].get(hand_type, 9)
            targets.append(group)
        elif 'target_group' in last_row and pd.notna(last_row['target_group']):
            # –ï—Å–ª–∏ –≥—Ä—É–ø–ø–∞ —É–∂–µ –≤—ã—á–∏—Å–ª–µ–Ω–∞
            targets.append(int(last_row['target_group']))
        else:
            # Default –≥—Ä—É–ø–ø–∞
            targets.append(9)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    unique_targets = np.unique(targets)
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã –≤ targets: {unique_targets}")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {dict(zip(*np.unique(targets, return_counts=True)))}")
    
    return targets

class PreflopRangeDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô)"""
    def __init__(self, sequences, feature_columns, targets, scaler=None):
        self.sequences = sequences
        self.feature_columns = feature_columns
        self.targets = targets  # –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
        self.scaler = scaler
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        self.valid_indices = []
        for i, seq in enumerate(sequences):
            if len(seq) > 0:
                self.valid_indices.append(i)
        
        print(f"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(self.valid_indices)} –∏–∑ {len(sequences)}")
        
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        seq_df = self.sequences[actual_idx]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ—Ñ–ª–æ–ø –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        preflop_mask = seq_df['Street_id'] == 0  # –ü—Ä–µ—Ñ–ª–æ–ø
        preflop_seq = seq_df[preflop_mask]
        
        if len(preflop_seq) == 0:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ—Ñ–ª–æ–ø –¥–∞–Ω–Ω—ã—Ö, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏
            preflop_seq = seq_df.iloc[:min(5, len(seq_df))]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        available_features = [col for col in self.feature_columns if col in preflop_seq.columns]
        features = preflop_seq[available_features].fillna(0).values
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler
        if self.scaler:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω—É–ª—è–º–∏
            if len(available_features) < len(self.feature_columns):
                full_features = np.zeros((len(features), len(self.feature_columns)))
                for i, col in enumerate(self.feature_columns):
                    if col in available_features:
                        col_idx = available_features.index(col)
                        full_features[:, i] = features[:, col_idx]
                features = full_features
            
            features = self.scaler.transform(features)
        
        features = features.astype(np.float32)
        
        # –ü–æ–∑–∏—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è –∏–≥—Ä–æ–∫–æ–≤
        positions = preflop_seq['Position_encoded'].values if 'Position_encoded' in preflop_seq else np.zeros(len(preflop_seq))
        actions = preflop_seq['Action_encoded'].values if 'Action_encoded' in preflop_seq else np.zeros(len(preflop_seq))
        
        # –¶–µ–ª–µ–≤—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        target_ranges = self.targets.get(actual_idx, {})
        
        return {
            'features': torch.tensor(features, dtype=torch.float32),
            'positions': torch.tensor(positions, dtype=torch.long),
            'actions': torch.tensor(actions, dtype=torch.long),
            'target_ranges': target_ranges,
            'seq_length': len(preflop_seq)
        }


class ProbableHandDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–∏–ª—ã —Ä—É–∫–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô)"""
    def __init__(self, sequences, feature_columns, range_predictions, targets, scaler=None, max_seq_length=50):
        self.sequences = sequences
        self.feature_columns = feature_columns
        self.range_predictions = range_predictions  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
        self.targets = targets  # –ì—Ä—É–ø–ø—ã 0-9 –∏–∑ HM3 –º–∞–ø–ø–∏–Ω–≥–∞
        self.scaler = scaler
        self.max_seq_length = max_seq_length
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
        self.valid_indices = []
        for i, seq in enumerate(sequences):
            if len(seq) > 0 and i < len(targets):
                self.valid_indices.append(i)
        
        print(f"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(self.valid_indices)} –∏–∑ {len(sequences)}")
        
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        seq_df = self.sequences[actual_idx]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        available_features = [col for col in self.feature_columns if col in seq_df.columns]
        features = seq_df[available_features].fillna(0).values
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler
        if self.scaler:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            if len(available_features) < len(self.feature_columns):
                full_features = np.zeros((len(features), len(self.feature_columns)))
                for i, col in enumerate(self.feature_columns):
                    if col in available_features:
                        col_idx = available_features.index(col)
                        full_features[:, i] = features[:, col_idx]
                features = full_features
            
            features = self.scaler.transform(features)
        
        features = features.astype(np.float32)
        
        # Padding/truncation
        seq_len = min(len(features), self.max_seq_length)
        if seq_len < self.max_seq_length:
            padding = np.zeros((self.max_seq_length - seq_len, features.shape[1]))
            features = np.vstack([features[:seq_len], padding])
        else:
            features = features[:self.max_seq_length]
        
        # –£–ª–∏—Ü—ã –∏ –¥–µ–π—Å—Ç–≤–∏—è
        streets = seq_df['Street_id'].values[:seq_len] if 'Street_id' in seq_df else np.zeros(seq_len)
        actions = seq_df['Action_encoded'].values[:seq_len] if 'Action_encoded' in seq_df else np.zeros(seq_len)
        
        # Padding –¥–ª—è streets –∏ actions
        if seq_len < self.max_seq_length:
            streets = np.pad(streets, (0, self.max_seq_length - seq_len), 'constant')
            actions = np.pad(actions, (0, self.max_seq_length - seq_len), 'constant')
        
        # –ú–∞—Å–∫–∞ –¥–ª—è padding
        mask = torch.zeros(self.max_seq_length, dtype=torch.bool)
        mask[seq_len:] = True
        
        # –¶–µ–ª–µ–≤–∞—è –≥—Ä—É–ø–ø–∞
        target_group = self.targets[actual_idx] if actual_idx < len(self.targets) else 0
        
        return {
            'features': torch.tensor(features, dtype=torch.float32),
            'range_predictions': self.range_predictions.get(actual_idx, {}),
            'streets': torch.tensor(streets, dtype=torch.long),
            'actions': torch.tensor(actions, dtype=torch.long),
            'mask': mask,
            'target_group': torch.tensor(target_group, dtype=torch.long),
            'seq_length': seq_len
        }
        
def train_preflop_model(data_dict, device):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–ª–æ–ø –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = PreflopRangeDataset(
        data_dict['train_sequences'],
        data_dict['feature_columns'],
        data_dict['preflop_targets'],
        data_dict['scaler']
    )
    
    if len(train_dataset) == 0:
        print("‚ùå –ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ—Ñ–ª–æ–ø –º–æ–¥–µ–ª–∏!")
        return None
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=16,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π batch size
        shuffle=True,
        collate_fn=collate_fn,
        drop_last=True  # –£–±–∏—Ä–∞–µ–º –Ω–µ–ø–æ–ª–Ω—ã–µ –±–∞—Ç—á–∏
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = PreflopRangeRWKV(
        input_dim=len(data_dict['feature_columns']),
        hidden_dim=128,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        num_layers=2     # –ú–µ–Ω—å—à–µ —Å–ª–æ–µ–≤
    ).to(device)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –±–æ–ª—å—à–∏–º learning rate
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    mse_criterion = nn.MSELoss()
    bce_criterion = nn.BCELoss()
    
    # –û–±—É—á–µ–Ω–∏–µ
    epochs = 20
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        batch_count = 0
        
        for batch in train_loader:
            if batch is None:
                continue
                
            features = batch['features'].to(device)
            positions = batch['positions'].to(device)
            actions = batch['actions'].to(device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            if features.size(0) == 0:
                continue
            
            optimizer.zero_grad()
            model.reset_states()
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            try:
                range_preds = model(features, positions, actions)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ forward pass: {e}")
                continue
            
            # –†–∞—Å—á–µ—Ç –ø–æ—Ç–µ—Ä—å
            loss = 0
            loss_count = 0
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø–æ—Ç–µ—Ä—å - –ø—Ä–æ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–∞—á–∞–ª–∞
            for batch_idx in range(features.size(0)):
                for pos in range(9):
                    if f'pos_{pos}' in range_preds:
                        pred = range_preds[f'pos_{pos}'][batch_idx]
                        
                        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ü–µ–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
                        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞—Å—Ç–æ—è—â–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
                        target_vector = torch.zeros(169).to(device)
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                        random_indices = torch.randint(0, 169, (20,))
                        target_vector[random_indices] = torch.rand(20).to(device)
                        target_vector = target_vector / target_vector.sum()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                        
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MSE –≤–º–µ—Å—Ç–æ BCE –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        loss += mse_criterion(pred, target_vector)
                        loss_count += 1
            
            if loss_count > 0:
                loss = loss / loss_count
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é
                l2_reg = 0
                for param in model.parameters():
                    l2_reg += torch.sum(param ** 2)
                loss += 0.001 * l2_reg
                
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                total_loss += loss.item()
                batch_count += 1
        
        avg_loss = total_loss / max(batch_count, 1)
        if epoch % 5 == 0:
            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
    
    return model


def collate_fn(batch):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π —Å None –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)"""
    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
    batch = [item for item in batch if item is not None]
    if len(batch) == 0:
        return None
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    max_len = max(item['seq_length'] for item in batch)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–ª—è –±–∞—Ç—á–∞
    padded_features = []
    padded_positions = []
    padded_actions = []
    target_ranges = []
    seq_lengths = []
    
    for item in batch:
        seq_len = item['seq_length']
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        features = item['features']
        positions = item['positions']
        actions = item['actions']
        
        # Padding features
        if seq_len < max_len:
            # –î–æ–±–∞–≤–ª—è–µ–º padding
            feat_padding = torch.zeros(max_len - seq_len, features.shape[1])
            features = torch.cat([features, feat_padding], dim=0)
            
            # Padding –¥–ª—è positions –∏ actions
            pos_padding = torch.zeros(max_len - seq_len, dtype=torch.long)
            positions = torch.cat([positions, pos_padding], dim=0)
            
            act_padding = torch.zeros(max_len - seq_len, dtype=torch.long)
            actions = torch.cat([actions, act_padding], dim=0)
        
        padded_features.append(features)
        padded_positions.append(positions)
        padded_actions.append(actions)
        target_ranges.append(item['target_ranges'])
        seq_lengths.append(seq_len)
    
    # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á
    batch_dict = {
        'features': torch.stack(padded_features),
        'positions': torch.stack(padded_positions),
        'actions': torch.stack(padded_actions),
        'target_ranges': target_ranges,
        'seq_lengths': torch.tensor(seq_lengths)
    }
    
    return batch_dict            